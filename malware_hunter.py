from bs4 import BeautifulSoup
import requests
import pandas as pd
import base64
from datetime import datetime, timedelta
import random
import sys
import argparse
import copy

def main():
    try:
        parser = argparse.ArgumentParser(
            description='Malware Hunter / Threat Intel Builder, Feedly Integration for AI feeds in Pro+. Uses Malpedia for information',
            epilog='If you use the "--search" argument without the "--d" or "--y" argument you will not get any description or yara information but you will get the name, last updated, and family link.\n')
        parser.add_argument('--get_malware', type=int, default=10, metavar='', help='Amount of Malware to Query, displayed from newest to oldest.')
        parser.add_argument('--search', type=str, nargs='+', metavar='', help='List of malware names to search for. Ex:"--search malware1 malware2 --d --y."')
        parser.add_argument('--ef', action='store_true', help='Encoded Feedly Filter URL from queried malware')
        parser.add_argument('--cf', action='store_true', help='Clear Text Feedly Filter from queried malware')
        parser.add_argument('--d', action='store_true', help='Display the description for the selected malware')
        parser.add_argument('--y', action='store_true', help='Display yara rule(s) for selected malware')
        parser.add_argument('--us', action='store_true', help='Select From queried maalware, meant to be used with --search, --d and or --y')
        parser.add_argument('--s', action='store_true', help='Silent mode, suppresses default output. Valid only when --ef, --cf, --d, or --y is used.')
        parser.add_argument('--year', type=int, default=None, metavar='', help='Filter malware by specific year (e.g., 2024, 2025). If not specified, shows current year.')
        parser.add_argument('--days', type=int, default=None, metavar='', help='Filter malware updated within the last N days.')

        args = parser.parse_args()

        if args.s and not any([args.ef, args.cf, args.d, args.y, args.search]):
            parser.error("[+] --s can only be used when --search, --ef, --cf, --d, or --y is provided.")
        
        just_malware_names, sorted_malware_list = top_malware_strains(args.get_malware, args.s, args.year, args.days)  # Passing additional parameters

        if args.ef:
            URL_Encoded_feedly_filter(just_malware_names)

        if args.cf:
            clear_text_feedly_filter(just_malware_names)

        if args.d or args.y:  # If either --d or --y is provided
            if not args.search:  # Only call user_select_malware if --search is not provided
                user_select_malware(sorted_malware_list, args.d, args.y)

        if args.search:
            search_malware(args.search, sorted_malware_list, args.d, args.y)

        if args.us:
            user_select_malware(sorted_malware_list, args.d, args.y)

    except KeyboardInterrupt:
        print("\n[+] Ctrl+C pressed. Exiting")
        sys.exit(0)

def top_malware_strains(get_malware, silent_mode, filter_year=None, filter_days=None):
    # Make the request and parse the HTML
    agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.1000.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36 Edg/93.0.961.52',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.9999.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.9999.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.9999.0 Safari/537.36',]

    user_agent = random.choice(agents) # Randomize user agents

    r = requests.get("https://malpedia.caad.fkie.fraunhofer.de/families", headers={'User-Agent': user_agent})
    soup = BeautifulSoup(r.content, 'html.parser')

    # Find the table with the specific class
    malware_table = soup.find('table', {'class': 'table enumerated table-dark table-sm'})

    # Initialize an empty list to store malware data
    malware_list = []
    family_link = "https://malpedia.caad.fkie.fraunhofer.de"
    
    # Get current date for filtering
    current_date = datetime.now()
    
    # Determine the year filter
    if filter_year is None:
        # Default to current year
        year_filter = str(current_date.year)
    else:
        year_filter = str(filter_year)
    
    # Iterate through each row in the table body
    for row in malware_table.find('tbody').find_all('tr'):
        data_href = family_link + row.get('data-href', 'No Data Href Found') 
        common_name = row.find('td', {'class': 'common_name'}).text.strip()
        last_updated = row.find('td', {'class': 'entry_updated'}).text.strip()
        alt_names = row.find('td', {'class': 'alt_names'}).text.strip()
       
        # Apply filtering based on parameters
        should_include = False
        
        if filter_days is not None:
            # Filter by days - include entries updated within the last N days
            try:
                update_date = datetime.strptime(last_updated, "%Y-%m-%d")
                days_diff = (current_date - update_date).days
                if days_diff <= filter_days:
                    should_include = True
            except ValueError:
                # If date parsing fails, skip this entry
                pass
        else:
            # Filter by year
            if last_updated.startswith(year_filter):
                should_include = True
        
        if should_include:
            malware_list.append({
                'Malware': common_name,
                'Last Updated': last_updated,
                'alt_names': alt_names,
                'data_href': data_href  # Add the scraped family_link to the malware data
            })
    
    all_malware_list = sorted(malware_list, key=lambda x: x['Last Updated'], reverse=True)

    all_names = []
    for i in all_malware_list:
        all_names.append((i['Malware'], i['Last Updated'], i['alt_names'], i['data_href']))

    sorted_malware_list = sorted(malware_list, key=lambda x: x['Last Updated'], reverse=True)[:get_malware]

    just_malware_names = []
    for i in sorted_malware_list:
        just_malware_names.append(i['Malware'])

    if not silent_mode:
        #print(just_malware_names) #uncomment for just a list of scraped malware names
        filter_info = ""
        if filter_days is not None:
            filter_info = f" (Last {filter_days} days)"
        else:
            filter_info = f" ({year_filter})"
            
        print("\n*******************************")
        print(f"*      New Malware Data{filter_info:^8}*")
        print("*******************************\n")
        
        if len(sorted_malware_list) == 0:
            print(f"[!] No malware entries found for the specified filter criteria.")
            print(f"[!] Try adjusting your filter (current year: {current_date.year})")
        else:
            # Print the sorted list
            for entry in sorted_malware_list[:get_malware]:
                alt_names_str = entry['alt_names'].replace("[", "").replace("]", "").replace("'", "")
                if alt_names_str == "":
                    print(f"Malware: {entry['Malware']} ---> Last Updated: {entry['Last Updated']} ---> Family Link: {entry['data_href']}")
                else:
                    print(f"Malware: {entry['Malware']} ---> Last Updated: {entry['Last Updated']} ---> alt_names: {alt_names_str} ---> Family Link: {entry['data_href']}") 

    return just_malware_names, sorted_malware_list

def search_malware(search_terms, sorted_malware_list, display_description, display_yara):
    print("\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
    print("@       Search Results        @")
    print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n")
    for term in search_terms:
        print("\n*******************************")
        print(f"*     Searching {term}       *")
        print("*******************************\n")
        print(f"[+] Searching for {term}:")
        found = False
        for entry in sorted_malware_list:
            if term.lower() in entry['Malware'].lower():
                print(f"Found: {entry['Malware']} ---> Last Updated: {entry['Last Updated']} ---> Family Link: {entry['data_href']}")
                found = True
                if display_description:
                    get_malware_description(entry['data_href'])
                if display_yara:
                    get_malware_yara(entry['data_href'])
        if not found:
            print(f"No results found for {term}")

def user_select_malware(sorted_malware_list, display_description, display_yara):
    while True:
        print("\n*******************************")
        print("*       Select Malware        *")
        print("*******************************\n")
        
        for index, entry in enumerate(sorted_malware_list, start=1):
            print(f"{index}. Malware: {entry['Malware']} ")

        continue_choice = input('\n[+] "Y" to continue or "N" to exit: ').strip().lower()
        if continue_choice == 'n':
            return sys.exit()
        
        while True:
            user_choice = input("\n[+] Enter the number corresponding to the malware you want to explore: ")
            try:
                user_choice = int(user_choice)
                if 1 <= user_choice <= len(sorted_malware_list):
                    selected_data_href = sorted_malware_list[user_choice - 1]['data_href']
                    
                    if display_description:  # If --d is provided
                        get_malware_description(selected_data_href)
                    if display_yara:  # If --y is provided
                        get_malware_yara(selected_data_href)

                    continue_choice = input("\nWould you like to see any others? (y/n): ").strip().lower()
                    if continue_choice == 'n':
                        return user_choice  # Return user_choice when user decides not to continue
                else:
                    print("[x] Invalid choice. Please enter a number between 1 and", len(sorted_malware_list))
            except ValueError:
                print(f"{user_choice} is not an integer\n")

def get_malware_description(data_href):
    agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.1000.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36 Edg/93.0.961.52',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.9999.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.9999.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.9999.0 Safari/537.36',]

    user_agent = random.choice(agents)
    # Make the request and parse the HTML
    r = requests.get(data_href, headers={'User-Agent': user_agent})
    soup = BeautifulSoup(r.content, 'html.parser')
    
    # Find the <div> tag with style="margin-top:5px"
    div_tag = soup.find('div', style="margin-top:5px") 
    
    # Initialize description as an empty string
    description = ''
    
    # If <div> tag is found, get all subsequent <p> tags
    if div_tag:
        p_tags = soup.find_all('p')
        div_index = p_tags.index(div_tag.find_next('p'))  # Find the index of the first <p> tag after the <div>
        
        # Concatenate the text of all <p> tags after the <div>
        for p_tag in p_tags[div_index:]:
            description += p_tag.text.strip() + ' '

    split_string = "If your designated proposal"
    description = description.split(split_string)[0]

    if description:
        print("\n*******************************")
        print("*         Description         *")
        print("*******************************")
        print("\n",description,"\n")
    else:
        print("\n*******************************")
        print("*  NO Description Available   *")
        print("*******************************")

def get_malware_yara(data_href):
    agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.1000.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36 Edg/93.0.961.52',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.9999.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.9999.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.9999.0 Safari/537.36',]

    user_agent = random.choice(agents)
    # Make the request and parse the HTML
    r = requests.get(data_href, headers={'User-Agent': user_agent})
    soup = BeautifulSoup(r.content, 'html.parser')

    yara_rule_tags = soup.find_all('pre', class_='mono-font')
    if yara_rule_tags:
        for yara_rule_tag in yara_rule_tags:
            yara_rule = yara_rule_tag.get_text(strip=True)
            print("\n*******************************")
            print("*          Yara rule          *")
            print("*******************************\n")
            print(yara_rule,'\n')
    else:
        print("\n*******************************")
        print("*   No Yara rule Available    *")
        print("*******************************\n")

def URL_Encoded_feedly_filter(flat_list):
 
    search = ''

    for i in flat_list:
        search += ',{"text":"' + i + '"}'

    feedly_filter = '{"layers":[{"parts":[{"type":"customKeyword"'+search+'],"salience":"mention","searchHint":"","type":"matches"}],"bundles":[]}' # create feedly filter

    count = 0
    removed = False  # A flag to indicate if the 4th '{' has been removed

    new_feedly_filter = '' # need to remove 4th instance of "{" and only the 4th instance
    for char in feedly_filter:
        if char == '{':
            count += 1
        if count == 4 and not removed:  
            removed = True  
            continue  
        new_feedly_filter += char

    new_feedly_filter = new_feedly_filter.replace(',{"text":"No alias"}', '') # Get rid of the "No alias" Tuple porttion from combined in feedly filter

    encoded_filter = base64.b64encode(new_feedly_filter.encode()).decode() # encode feedly filter to b64 

    feedly_filter_url = f"https://feedly.com/i/aiFeeds?options={encoded_filter}" 
    
    print("\n*******************************")
    print("*  Encoded Feedly Filter URL  *")
    print("*******************************\n")
    print(feedly_filter_url)

def clear_text_feedly_filter(flat_list):
 
    search = ''

    for i in flat_list:
        search += ',{"text":"' + i + '"}'

    feedly_filter = '{"layers":[{"parts":[{"type":"customKeyword"'+search+'],"salience":"mention","searchHint":"","type":"matches"}],"bundles":[]}' # create feedly filter

    count = 0
    removed = False  # A flag to indicate if the 4th '{' has been removed

    new_feedly_filter = '' # need to remove 4th instance of "{" and only the 4th instance
    for char in feedly_filter:
        if char == '{':
            count += 1
        if count == 4 and not removed:  
            removed = True  
            continue  
        new_feedly_filter += char

    new_feedly_filter = new_feedly_filter.replace(',{"text":"No alias"}', '') # Get rid of the "No alias" Tuple porttion from combined in feedly filter

    print("\n*******************************")
    print("*  Clear Text Feedly Filter   *")
    print("*******************************\n")
    print(new_feedly_filter)


if __name__ == "__main__":
    main()
